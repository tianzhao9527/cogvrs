# ğŸ§  Cogvrs ç³»ç»Ÿæ·±åº¦è§£æ - ç®—æ³•æœºåˆ¶ä¸åŸç†

## ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡è¯¦è§£ä¸ç®—æ³•æœºåˆ¶

### 1. ğŸ§¬ æ™ºèƒ½ä½“ç”Ÿå‘½ç³»ç»Ÿ

#### ğŸ”¢ **Agents (æ™ºèƒ½ä½“æ•°é‡)**
```python
# ç»Ÿè®¡é€»è¾‘
alive_agents = [agent for agent in self.agents if agent.alive]
agent_count = len(alive_agents)
```

**å«ä¹‰**: å½“å‰å­˜æ´»çš„AIæ™ºèƒ½ä½“æ€»æ•°
**æœºåˆ¶**: 
- æ¯ä¸ªæ™ºèƒ½ä½“æœ‰`alive`å¸ƒå°”å±æ€§
- æ­»äº¡æ¡ä»¶ï¼š`energy <= 0` æˆ– `health <= 0` æˆ– `age > max_lifespan`
- åŠ¨æ€å˜åŒ–ï¼šå‡ºç”Ÿ(ç¹æ®–)å¢åŠ ï¼Œæ­»äº¡å‡å°‘

**èƒŒåç®—æ³•**:
```python
# ç”Ÿå‘½çŠ¶æ€åˆ¤æ–­ (simple_agent.py)
def update_vital_signs(self, dt):
    # èƒ½é‡æ¶ˆè€—
    self.energy -= self.base_metabolism * dt
    
    # å¥åº·è¡°å‡ï¼ˆå¹´é¾„ç›¸å…³ï¼‰
    aging_factor = min(self.age / 200.0, 1.0)
    self.health -= aging_factor * 0.1 * dt
    
    # æ­»äº¡åˆ¤å®š
    if self.energy <= 0 or self.health <= 0:
        self.alive = False
```

---

### 2. âš¡ **Avg Energy (å¹³å‡èƒ½é‡)**
```python
# è®¡ç®—é€»è¾‘
energies = [agent.energy for agent in alive_agents]
avg_energy = sum(energies) / len(energies) if energies else 0
```

**å«ä¹‰**: æ‰€æœ‰å­˜æ´»æ™ºèƒ½ä½“çš„å¹³å‡ç”Ÿå‘½èƒ½é‡
**èŒƒå›´**: 0 - 150 (æœ€å¤§èƒ½é‡é™åˆ¶)
**æœºåˆ¶**:

#### ğŸ”‹ èƒ½é‡ç³»ç»Ÿç®—æ³•
```python
class EnergySystem:
    def __init__(self):
        self.initial_energy = 100.0
        self.max_energy = 150.0
        self.base_metabolism = 0.5  # åŸºç¡€ä»£è°¢ç‡
        
    def update_energy(self, agent, dt):
        # 1. åŸºç¡€ä»£è°¢æ¶ˆè€—
        metabolic_cost = self.base_metabolism * dt
        
        # 2. æ´»åŠ¨æ¶ˆè€—ï¼ˆåŸºäºç§»åŠ¨é€Ÿåº¦ï¼‰
        movement_cost = agent.velocity.magnitude() * 0.1 * dt
        
        # 3. ç¥ç»ç½‘ç»œæ€è€ƒæ¶ˆè€—
        neural_cost = agent.brain.get_computation_cost() * dt
        
        # 4. ç¤¾äº¤äº’åŠ¨æ¶ˆè€—
        social_cost = agent.social_interactions * 0.01 * dt
        
        # æ€»æ¶ˆè€—
        total_cost = metabolic_cost + movement_cost + neural_cost + social_cost
        agent.energy -= total_cost
        
    def gain_energy(self, agent, food_value):
        # è§…é£Ÿè·å¾—èƒ½é‡
        energy_gain = food_value * agent.get_digestion_efficiency()
        agent.energy = min(agent.energy + energy_gain, self.max_energy)
```

**çŠ¶æ€åˆ¤å®š**:
- **ğŸ”´ å±é™© (0-30)**: é¢ä¸´æ­»äº¡ï¼Œè¡Œä¸ºå˜ä¸ºæ€¥è¿«è§…é£Ÿ
- **ğŸŸ¡ ä¸€èˆ¬ (30-70)**: éœ€è¦è¡¥å……èƒ½é‡ï¼Œå¹³è¡¡è§…é£Ÿå’Œå…¶ä»–æ´»åŠ¨
- **ğŸŸ¢ å¥åº· (70-100)**: æ­£å¸¸æ´»åŠ¨ï¼Œå¯ä»¥è¿›è¡Œç¤¾äº¤å’Œæ¢ç´¢
- **ğŸ”µ ä¼˜ç§€ (100-150)**: èƒ½é‡å……æ²›ï¼Œå¯ä»¥ç¹æ®–å’Œå¸®åŠ©ä»–äºº

---

### 3. ğŸ“Š **Avg Age (å¹³å‡å¹´é¾„)**
```python
# è®¡ç®—é€»è¾‘  
ages = [agent.age for agent in alive_agents]
avg_age = sum(ages) / len(ages) if ages else 0
```

**å«ä¹‰**: æ™ºèƒ½ä½“çš„ç”Ÿå­˜æ—¶é—´ï¼Œä»¥æ¨¡æ‹Ÿæ­¥æ•°è®¡ç®—
**æœºåˆ¶**:

#### ğŸ•°ï¸ å¹´é¾„ç³»ç»Ÿç®—æ³•
```python
def update_age(self, dt):
    # å¹´é¾„å¢é•¿
    self.age += dt
    
    # å¹´é¾„å¯¹å„é¡¹èƒ½åŠ›çš„å½±å“
    def get_age_factor(self):
        if self.age < 20:      # å¹¼å¹´æœŸ
            return 0.8  # 80%èƒ½åŠ›
        elif self.age < 100:   # æˆå¹´æœŸ  
            return 1.0  # 100%èƒ½åŠ›
        elif self.age < 200:   # ä¸­å¹´æœŸ
            return 0.9  # 90%èƒ½åŠ›
        else:                  # è€å¹´æœŸ
            return 0.7  # 70%èƒ½åŠ›
    
    # å¹´é¾„å½±å“å­¦ä¹ èƒ½åŠ›
    def get_learning_rate(self):
        base_rate = 0.01
        if self.age < 50:
            return base_rate * 1.5  # å¹´è½»æ—¶å­¦ä¹ å¿«
        else:
            return base_rate * (1.0 - (self.age - 50) / 300)
```

**ç”Ÿå‘½é˜¶æ®µ**:
- **0-20æ­¥**: ğŸ¼ å¹¼å¹´æœŸ - å­¦ä¹ èƒ½åŠ›å¼ºï¼Œæ— ç¹æ®–èƒ½åŠ›
- **20-100æ­¥**: ğŸ’ª æˆå¹´æœŸ - å„é¡¹èƒ½åŠ›å·…å³°ï¼Œæœ€ä½³ç¹æ®–æœŸ
- **100-200æ­¥**: ğŸ§“ ä¸­å¹´æœŸ - ç»éªŒä¸°å¯Œï¼Œç¹æ®–èƒ½åŠ›ä¸‹é™
- **200+æ­¥**: ğŸ‘´ è€å¹´æœŸ - è¡°è€åŠ é€Ÿï¼Œæ­»äº¡æ¦‚ç‡å¢åŠ 

---

### 4. â¤ï¸ **Avg Health (å¹³å‡å¥åº·)**
```python
# è®¡ç®—é€»è¾‘
healths = [agent.health for agent in alive_agents] 
avg_health = sum(healths) / len(healths) if healths else 0
```

**å«ä¹‰**: æ™ºèƒ½ä½“èº«ä½“å¥åº·çŠ¶å†µï¼Œå½±å“æ‰€æœ‰æ´»åŠ¨æ•ˆç‡
**èŒƒå›´**: 0 - 100

#### ğŸ¥ å¥åº·ç³»ç»Ÿç®—æ³•
```python
class HealthSystem:
    def update_health(self, agent, dt):
        # 1. å¹´é¾„ç›¸å…³è¡°å‡
        aging_decay = self.calculate_aging_decay(agent.age) * dt
        
        # 2. èƒ½é‡æ°´å¹³å½±å“
        energy_factor = self.calculate_energy_health_factor(agent.energy)
        
        # 3. å‹åŠ›å½±å“ï¼ˆç¤¾äº¤å‹åŠ›ã€èµ„æºç«äº‰ï¼‰
        stress_factor = self.calculate_stress_factor(agent)
        
        # 4. è¿åŠ¨å½±å“ï¼ˆé€‚åº¦è¿åŠ¨æœ‰ç›Šå¥åº·ï¼‰
        exercise_factor = self.calculate_exercise_factor(agent.velocity)
        
        # å¥åº·å˜åŒ–
        health_change = (-aging_decay + energy_factor - stress_factor + exercise_factor) * dt
        agent.health = max(0, min(100, agent.health + health_change))
    
    def calculate_aging_decay(self, age):
        # å¹´é¾„è¡°å‡å‡½æ•°ï¼šæŒ‡æ•°å¢é•¿
        return 0.1 * (1 + (age / 100) ** 2)
    
    def calculate_energy_health_factor(self, energy):
        # èƒ½é‡å¯¹å¥åº·çš„å½±å“
        if energy > 80:
            return 0.2   # èƒ½é‡å……è¶³ï¼Œå¥åº·æ”¹å–„
        elif energy < 30:
            return -0.5  # èƒ½é‡ä¸è¶³ï¼Œå¥åº·æ¶åŒ–
        else:
            return 0     # ä¸­ç­‰èƒ½é‡ï¼Œå¥åº·ç¨³å®š
    
    def calculate_stress_factor(self, agent):
        # å‹åŠ›è®¡ç®—
        competition_stress = len(agent.nearby_agents) * 0.01
        resource_stress = max(0, 0.1 - agent.local_resource_density)
        return competition_stress + resource_stress
```

**å¥åº·çŠ¶æ€åˆ¤å®š**:
- **90-100**: ğŸŸ¢ ä¼˜ç§€ - æ‰€æœ‰æ´»åŠ¨æ•ˆç‡100%
- **70-89**: ğŸŸ¡ è‰¯å¥½ - æ´»åŠ¨æ•ˆç‡90%
- **50-69**: ğŸŸ  ä¸€èˆ¬ - æ´»åŠ¨æ•ˆç‡75%
- **30-49**: ğŸ”´ ä¸ä½³ - æ´»åŠ¨æ•ˆç‡50%
- **0-29**: âš« å±é‡ - æ´»åŠ¨æ•ˆç‡25%ï¼Œé¢ä¸´æ­»äº¡

---

### 5. ğŸ‘¶ **Offspring (åä»£æ€»æ•°)**
```python
# è®¡ç®—é€»è¾‘
total_offspring = sum(agent.offspring_count for agent in alive_agents)
```

**å«ä¹‰**: å½“å‰æ‰€æœ‰æ™ºèƒ½ä½“äº§ç”Ÿçš„åä»£ç´¯è®¡æ•°é‡
**æœºåˆ¶**:

#### ğŸ§¬ ç¹æ®–ç³»ç»Ÿç®—æ³•
```python
class ReproductionSystem:
    def check_reproduction_conditions(self, agent1, agent2):
        # ç¹æ®–æ¡ä»¶æ£€æŸ¥
        conditions = [
            agent1.energy > 80,           # èƒ½é‡å……è¶³
            agent2.energy > 80,
            agent1.age > 50,              # æ€§æˆç†Ÿ
            agent2.age > 50,
            agent1.health > 70,           # å¥åº·è‰¯å¥½
            agent2.health > 70,
            agent1.offspring_count < 3,   # ç¹æ®–æ¬¡æ•°é™åˆ¶
            agent2.offspring_count < 3,
            self.calculate_distance(agent1, agent2) < 5,  # è·ç¦»è¶³å¤Ÿè¿‘
            self.population_size < self.max_population     # ç§ç¾¤æœªè¿‡è½½
        ]
        return all(conditions)
    
    def reproduce(self, parent1, parent2):
        # åˆ›å»ºå­ä»£
        child_config = self.create_child_config(parent1, parent2)
        child = SimpleAgent(child_config)
        
        # é—ä¼ ç®—æ³• - ç¥ç»ç½‘ç»œæƒé‡æ··åˆ
        child.brain = self.crossover_neural_networks(
            parent1.brain, parent2.brain
        )
        
        # çªå˜
        child.brain.mutate(mutation_rate=0.1)
        
        # æ›´æ–°çˆ¶æ¯çŠ¶æ€
        parent1.offspring_count += 1
        parent2.offspring_count += 1
        parent1.energy -= 30  # ç¹æ®–æ¶ˆè€—
        parent2.energy -= 30
        
        return child
    
    def crossover_neural_networks(self, brain1, brain2):
        # ç¥ç»ç½‘ç»œäº¤å‰ç®—æ³•
        new_brain = NeuralBrain(brain1.config)
        
        for layer_idx in range(len(brain1.layers)):
            # éšæœºé€‰æ‹©æ¯å±‚æƒé‡æ¥æº
            if random.random() < 0.5:
                new_brain.layers[layer_idx].weights = brain1.layers[layer_idx].weights.copy()
            else:
                new_brain.layers[layer_idx].weights = brain2.layers[layer_idx].weights.copy()
                
            # æƒé‡å¹³å‡æ··åˆ
            alpha = random.random()
            new_brain.layers[layer_idx].weights = (
                alpha * brain1.layers[layer_idx].weights + 
                (1-alpha) * brain2.layers[layer_idx].weights
            )
        
        return new_brain
```

**ç¹æ®–æˆåŠŸç‡å½±å“å› ç´ **:
- **çˆ¶æ¯èƒ½é‡**: å¿…é¡»>80
- **çˆ¶æ¯å¹´é¾„**: å¿…é¡»>50æ­¥ï¼ˆæ€§æˆç†Ÿï¼‰
- **å¥åº·çŠ¶å†µ**: å¿…é¡»>70
- **ç§ç¾¤å¯†åº¦**: è¿‡åº¦æ‹¥æŒ¤ä¼šæŠ‘åˆ¶ç¹æ®–
- **åŸºå› å…¼å®¹æ€§**: é—ä¼ è·ç¦»å½±å“æˆåŠŸç‡

---

### 6. ğŸ¤ **Interactions (ç¤¾äº¤äº’åŠ¨)**
```python
# è®¡ç®—é€»è¾‘
total_interactions = sum(agent.social_interactions for agent in alive_agents)
```

**å«ä¹‰**: æ™ºèƒ½ä½“é—´ç´¯è®¡äº¤æµäº’åŠ¨æ¬¡æ•°
**æœºåˆ¶**:

#### ğŸ‘¥ ç¤¾äº¤ç³»ç»Ÿç®—æ³•
```python
class SocialSystem:
    def detect_interactions(self, agent, nearby_agents):
        interactions = 0
        
        for other_agent in nearby_agents:
            if self.should_interact(agent, other_agent):
                interaction_type = self.determine_interaction_type(agent, other_agent)
                self.execute_interaction(agent, other_agent, interaction_type)
                interactions += 1
        
        agent.social_interactions += interactions
        return interactions
    
    def should_interact(self, agent1, agent2):
        # ç¤¾äº¤æ„æ„¿è®¡ç®—
        distance = agent1.position.distance_to(agent2.position)
        if distance > agent1.perception_radius:
            return False
            
        # ç¤¾äº¤åŠ¨æœºå¼ºåº¦
        social_motivation = agent1.behavior.get_motivation_strength('social')
        energy_threshold = 40  # èƒ½é‡ä¸è¶³æ—¶å‡å°‘ç¤¾äº¤
        
        # ç›¸ä¼¼æ€§å¸å¼•ï¼ˆåŸºäºç¥ç»ç½‘ç»œæƒé‡ç›¸ä¼¼åº¦ï¼‰
        similarity = self.calculate_neural_similarity(agent1.brain, agent2.brain)
        
        # ç¤¾äº¤æ¦‚ç‡
        interaction_probability = (
            social_motivation * 
            (agent1.energy / 100) * 
            (1 + similarity) * 
            random.random()
        )
        
        return interaction_probability > 0.3
    
    def determine_interaction_type(self, agent1, agent2):
        # æ ¹æ®çŠ¶æ€å’Œæ€§æ ¼ç¡®å®šäº’åŠ¨ç±»å‹
        if agent1.energy < 30 or agent2.energy < 30:
            return "competition"  # èµ„æºç«äº‰
        elif agent1.health > 80 and agent2.health > 80:
            return "cooperation"  # åˆä½œäº’åŠ©
        else:
            return "information_exchange"  # ä¿¡æ¯äº¤æ¢
    
    def execute_interaction(self, agent1, agent2, interaction_type):
        if interaction_type == "cooperation":
            # åˆä½œï¼šåˆ†äº«èµ„æºä¿¡æ¯ï¼Œå°å¹…æå‡åŒæ–¹å¥åº·
            self.share_resource_information(agent1, agent2)
            agent1.health = min(100, agent1.health + 1)
            agent2.health = min(100, agent2.health + 1)
            
        elif interaction_type == "competition":
            # ç«äº‰ï¼šèƒ½é‡æ¶ˆè€—ï¼Œä½†å¯èƒ½è·å¾—èµ„æºè®¿é—®æƒ
            agent1.energy -= 2
            agent2.energy -= 2
            
        elif interaction_type == "information_exchange":
            # ä¿¡æ¯äº¤æ¢ï¼šå­¦ä¹ å¯¹æ–¹çš„æˆåŠŸç­–ç•¥
            self.neural_knowledge_transfer(agent1, agent2)
    
    def neural_knowledge_transfer(self, agent1, agent2):
        # ç¥ç»ç½‘ç»œçŸ¥è¯†è½¬ç§»
        transfer_rate = 0.01
        
        # é€‰æ‹©æ€§å­¦ä¹ ï¼šåªå­¦ä¹ å¯¹æ–¹æ›´æˆåŠŸçš„ç­–ç•¥
        if agent2.age > agent1.age:  # ç»éªŒæ›´ä¸°å¯Œ
            for layer in agent1.brain.layers:
                layer.weights += transfer_rate * (
                    agent2.brain.layers[layer.index].weights - layer.weights
                )
```

**ç¤¾äº¤è¡Œä¸ºæ¨¡å¼**:
- **ğŸ¤ åˆä½œ**: èƒ½é‡å……è¶³æ—¶ï¼Œåˆ†äº«ä¿¡æ¯ï¼Œäº’åŠ©æå‡
- **âš”ï¸ ç«äº‰**: èµ„æºç¨€ç¼ºæ—¶ï¼Œäº‰å¤ºè®¿é—®æƒ
- **ğŸ“š å­¦ä¹ **: å¹´è½»æ™ºèƒ½ä½“å‘å¹´é•¿è€…å­¦ä¹ ç»éªŒ
- **ğŸ’• æ±‚å¶**: æ»¡è¶³ç¹æ®–æ¡ä»¶æ—¶çš„é…å¯¹è¡Œä¸º

---

### 7. ğŸ’ **Resources (èµ„æºæ•°é‡)**
```python
# è®¡ç®—é€»è¾‘
world_state = self.world.get_world_state()
resource_count = world_state['num_resources']
```

**å«ä¹‰**: ç¯å¢ƒä¸­å¯ç”¨èµ„æºç‚¹çš„æ•°é‡
**æœºåˆ¶**:

#### ğŸŒ èµ„æºç³»ç»Ÿç®—æ³•
```python
class ResourceSystem:
    def __init__(self, world_size, density=0.15):
        self.world_size = world_size
        self.density = density
        self.resources = []
        self.regeneration_rate = 0.1
        
    def generate_resources(self):
        # èµ„æºç”Ÿæˆ
        total_resources = int(self.world_size[0] * self.world_size[1] * self.density)
        
        for _ in range(total_resources):
            resource = Resource(
                position=Vector2D(
                    random.uniform(0, self.world_size[0]),
                    random.uniform(0, self.world_size[1])
                ),
                value=random.uniform(10, 50),
                type=random.choice(['food', 'energy', 'material'])
            )
            self.resources.append(resource)
    
    def update_resources(self, dt):
        # èµ„æºå†ç”Ÿ
        for resource in self.resources:
            if resource.is_depleted():
                # å†ç”Ÿæ¦‚ç‡
                if random.random() < self.regeneration_rate * dt:
                    resource.regenerate()
        
        # æ–°èµ„æºè‡ªç„¶ç”Ÿæˆ
        if len(self.resources) < self.get_max_resources():
            if random.random() < 0.01 * dt:  # 1%æ¦‚ç‡æ¯ç§’
                self.spawn_new_resource()
    
    def consume_resource(self, agent, resource):
        # èµ„æºæ¶ˆè€—æœºåˆ¶
        if resource.is_available():
            consumption_efficiency = agent.get_consumption_efficiency()
            consumed_value = resource.value * consumption_efficiency
            
            # æ ¹æ®èµ„æºç±»å‹ç»™äºˆä¸åŒæ•ˆæœ
            if resource.type == 'food':
                agent.energy += consumed_value * 0.8
                agent.health += consumed_value * 0.2
            elif resource.type == 'energy':
                agent.energy += consumed_value
            elif resource.type == 'material':
                agent.health += consumed_value * 0.5
                # ææ–™å¯ç”¨äºæ”¹å–„ä½æ‰€ï¼ˆæœªæ¥æ‰©å±•ï¼‰
            
            resource.consume(consumed_value)
            return consumed_value
        
        return 0
```

**èµ„æºç±»å‹ä¸æ•ˆæœ**:
- **ğŸ é£Ÿç‰©**: ä¸»è¦æ¢å¤èƒ½é‡ï¼Œå°‘é‡æ¢å¤å¥åº·
- **âš¡ èƒ½é‡**: ç›´æ¥è¡¥å……ç”Ÿå‘½èƒ½é‡
- **ğŸ”§ ææ–™**: ä¸»è¦æ”¹å–„å¥åº·ï¼Œå¯ç”¨äºå»ºè®¾

**èµ„æºåˆ†å¸ƒå½±å“**:
- **é«˜å¯†åº¦åŒºåŸŸ**: ç«äº‰æ¿€çƒˆï¼Œç¤¾äº¤æ´»è·ƒ
- **ä½å¯†åº¦åŒºåŸŸ**: è§…é£Ÿå›°éš¾ï¼Œè¿ç§»è¡Œä¸º
- **èµ„æºæ¯ç«­**: è§¦å‘é›†ä½“è¿ç§»å’Œæ¿€çƒˆç«äº‰

---

### 8. ğŸ¯ **FPS (å¸§ç‡)**
```python
# è®¡ç®—é€»è¾‘
time_stats = self.time_manager.get_time_stats()
current_fps = time_stats['actual_fps']
```

**å«ä¹‰**: ç³»ç»Ÿæ¸²æŸ“æ€§èƒ½æŒ‡æ ‡ï¼Œå½±å“è§‚å¯Ÿä½“éªŒ
**æœºåˆ¶**:

#### âš¡ æ€§èƒ½ç›‘æ§ç®—æ³•
```python
class PerformanceMonitor:
    def __init__(self):
        self.frame_times = []
        self.target_fps = 30
        
    def update_fps(self, frame_time):
        self.frame_times.append(frame_time)
        if len(self.frame_times) > 60:  # ä¿æŒæœ€è¿‘60å¸§
            self.frame_times.pop(0)
        
        # è®¡ç®—å¹³å‡FPS
        avg_frame_time = sum(self.frame_times) / len(self.frame_times)
        self.current_fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0
        
    def get_performance_status(self):
        if self.current_fps > 25:
            return "excellent"
        elif self.current_fps > 20:
            return "good"  
        elif self.current_fps > 15:
            return "fair"
        else:
            return "poor"
```

---

## ğŸ¨ é¢œè‰²ç¼–ç ç³»ç»Ÿ

### ğŸ§  æ™ºèƒ½ä½“é¢œè‰²å«ä¹‰

#### åŸºäºå¥åº·çŠ¶æ€çš„é¢œè‰²æ˜ å°„
```python
def get_agent_color(agent):
    # å¥åº·åº¦é¢œè‰²æ˜ å°„
    health_ratio = agent.health / 100.0
    energy_ratio = agent.energy / agent.max_energy
    
    # ç»¼åˆå¥åº·æŒ‡æ•°
    vitality = (health_ratio + energy_ratio) / 2.0
    
    if vitality > 0.8:
        return (0, 255, 0)      # ğŸŸ¢ ç»¿è‰² - ä¼˜ç§€çŠ¶æ€
    elif vitality > 0.6:
        return (128, 255, 0)    # ğŸŸ¡ é»„ç»¿ - è‰¯å¥½çŠ¶æ€  
    elif vitality > 0.4:
        return (255, 255, 0)    # ğŸŸ¡ é»„è‰² - ä¸€èˆ¬çŠ¶æ€
    elif vitality > 0.2:
        return (255, 128, 0)    # ğŸŸ  æ©™è‰² - ä¸ä½³çŠ¶æ€
    else:
        return (255, 0, 0)      # ğŸ”´ çº¢è‰² - å±é™©çŠ¶æ€
```

#### ç‰¹æ®ŠçŠ¶æ€é¢œè‰²
```python
def get_special_state_color(agent):
    # ç¹æ®–çŠ¶æ€
    if agent.is_ready_for_reproduction():
        return (255, 0, 255)    # ğŸ’œ ç´«è‰² - æ±‚å¶æœŸ
    
    # å­¦ä¹ çŠ¶æ€  
    if agent.is_learning():
        return (0, 255, 255)    # ğŸ”µ é’è‰² - å­¦ä¹ ä¸­
    
    # ç¤¾äº¤æ´»è·ƒçŠ¶æ€
    if agent.recent_interactions > 5:
        return (0, 0, 255)      # ğŸ”µ è“è‰² - ç¤¾äº¤æ´»è·ƒ
    
    # è§…é£ŸçŠ¶æ€
    if agent.current_goal == "foraging":
        return (255, 255, 255)  # âšª ç™½è‰² - è§…é£Ÿä¸­
```

### ğŸŒ ç¯å¢ƒå…ƒç´ é¢œè‰²

#### èµ„æºé¢œè‰²ç¼–ç 
```python
def get_resource_color(resource):
    if resource.type == 'food':
        # é£Ÿç‰©ï¼šç»¿è‰²ç³»ï¼Œæ˜åº¦è¡¨ç¤ºä»·å€¼
        value_ratio = resource.value / resource.max_value
        green_intensity = int(128 + 127 * value_ratio)
        return (0, green_intensity, 0)
        
    elif resource.type == 'energy':
        # èƒ½é‡ï¼šè“è‰²ç³»
        return (0, 100, 255)
        
    elif resource.type == 'material':
        # ææ–™ï¼šæ£•è‰²ç³»
        return (139, 69, 19)
```

#### è½¨è¿¹é¢œè‰²
```python
def get_trajectory_color(agent):
    # åŸºäºæ™ºèƒ½ä½“å¹´é¾„çš„è½¨è¿¹é¢œè‰²
    age_ratio = min(agent.age / 200.0, 1.0)
    
    # å¹´è½»â†’è€å¹´ï¼šè“è‰²â†’çº¢è‰²æ¸å˜
    red = int(255 * age_ratio)
    blue = int(255 * (1 - age_ratio))
    
    return (red, 100, blue)
```

---

## ğŸ§® æ ¸å¿ƒç®—æ³•æ¶æ„

### 1. ğŸ§  ç¥ç»ç½‘ç»œå†³ç­–ç³»ç»Ÿ
```python
class NeuralBrain:
    """å¤šå±‚æ„ŸçŸ¥æœºç¥ç»ç½‘ç»œ"""
    
    def __init__(self, config):
        self.input_size = config['input_size']      # 20ä¸ªè¾“å…¥ç¥ç»å…ƒ
        self.hidden_sizes = config['hidden_sizes']  # [32, 16] éšè—å±‚
        self.output_size = config['output_size']    # 8ä¸ªè¾“å‡ºç¥ç»å…ƒ
        
        # æƒé‡åˆå§‹åŒ–ï¼šXavieråˆå§‹åŒ–
        self.layers = self._initialize_layers()
    
    def forward(self, inputs):
        """å‰å‘ä¼ æ’­"""
        activation = np.array(inputs)
        
        for layer in self.layers:
            # çº¿æ€§å˜æ¢
            z = np.dot(activation, layer.weights) + layer.bias
            # æ¿€æ´»å‡½æ•°ï¼ˆTanhï¼‰
            activation = np.tanh(z)
            
        return activation
    
    def backward(self, inputs, targets, learning_rate):
        """åå‘ä¼ æ’­å­¦ä¹ """
        # è®¡ç®—æ¢¯åº¦
        gradients = self._compute_gradients(inputs, targets)
        
        # æ›´æ–°æƒé‡
        for i, layer in enumerate(self.layers):
            layer.weights -= learning_rate * gradients[i]['weights']
            layer.bias -= learning_rate * gradients[i]['bias']
```

#### ğŸ¯ å†³ç­–è¾“å…¥ç¼–ç  (20ç»´è¾“å…¥å‘é‡)
```python
def encode_world_state(agent, world_state, nearby_agents, nearby_resources):
    inputs = np.zeros(20)
    
    # è‡ªèº«çŠ¶æ€ (6ç»´)
    inputs[0] = agent.energy / agent.max_energy           # èƒ½é‡æ°´å¹³
    inputs[1] = agent.health / 100.0                      # å¥åº·çŠ¶å†µ  
    inputs[2] = agent.age / 200.0                         # å¹´é¾„æ¯”ä¾‹
    inputs[3] = len(nearby_agents) / 10.0                 # ç¤¾äº¤å¯†åº¦
    inputs[4] = len(nearby_resources) / 5.0               # èµ„æºå¯†åº¦
    inputs[5] = agent.social_interactions / 100.0         # ç¤¾äº¤ç»éªŒ
    
    # æœ€è¿‘èµ„æºä¿¡æ¯ (6ç»´)
    if nearby_resources:
        closest_resource = min(nearby_resources, key=lambda r: agent.position.distance_to(r.position))
        inputs[6] = closest_resource.position.x / world_state['width']
        inputs[7] = closest_resource.position.y / world_state['height']  
        inputs[8] = closest_resource.value / 50.0
        inputs[9] = agent.position.distance_to(closest_resource.position) / 20.0
        inputs[10] = 1.0 if closest_resource.type == 'food' else 0.0
        inputs[11] = 1.0 if closest_resource.type == 'energy' else 0.0
    
    # æœ€è¿‘æ™ºèƒ½ä½“ä¿¡æ¯ (6ç»´)
    if nearby_agents:
        closest_agent = min(nearby_agents, key=lambda a: agent.position.distance_to(a.position))
        inputs[12] = closest_agent.position.x / world_state['width']
        inputs[13] = closest_agent.position.y / world_state['height']
        inputs[14] = closest_agent.energy / closest_agent.max_energy
        inputs[15] = agent.position.distance_to(closest_agent.position) / 20.0
        inputs[16] = 1.0 if closest_agent.energy > agent.energy else 0.0
        inputs[17] = closest_agent.age / 200.0
    
    # ç¯å¢ƒä¿¡æ¯ (2ç»´)
    inputs[18] = world_state['time_of_day']               # æ˜¼å¤œå‘¨æœŸ
    inputs[19] = world_state['resource_regeneration_rate'] # èµ„æºå†ç”Ÿç‡
    
    return inputs
```

#### ğŸ¯ å†³ç­–è¾“å‡ºè§£ç  (8ç»´è¾“å‡ºå‘é‡)
```python
def decode_neural_output(outputs):
    """å°†ç¥ç»ç½‘ç»œè¾“å‡ºè½¬æ¢ä¸ºå…·ä½“è¡ŒåŠ¨"""
    
    # ç§»åŠ¨å†³ç­– (2ç»´)
    move_x = np.tanh(outputs[0]) * 2.0  # Xæ–¹å‘ç§»åŠ¨ [-2, 2]
    move_y = np.tanh(outputs[1]) * 2.0  # Yæ–¹å‘ç§»åŠ¨ [-2, 2]
    
    # è¡Œä¸ºå†³ç­– (6ç»´ï¼Œä½¿ç”¨softmaxå½’ä¸€åŒ–)
    behavior_probs = softmax(outputs[2:8])
    behaviors = ['explore', 'forage', 'social', 'rest', 'reproduce', 'avoid']
    
    primary_behavior = behaviors[np.argmax(behavior_probs)]
    
    return {
        'movement': Vector2D(move_x, move_y),
        'behavior': primary_behavior,
        'behavior_confidence': np.max(behavior_probs)
    }
```

### 2. ğŸ§  è®°å¿†ç³»ç»Ÿæ¶æ„
```python
class MemorySystem:
    """ä¸‰å±‚è®°å¿†æ¶æ„"""
    
    def __init__(self, config):
        # å·¥ä½œè®°å¿†ï¼šçŸ­æœŸï¼Œå®¹é‡å°ï¼Œå¿«é€Ÿè®¿é—®
        self.working_memory = WorkingMemory(capacity=7)
        
        # é•¿æœŸè®°å¿†ï¼šæ°¸ä¹…å­˜å‚¨ï¼Œå®¹é‡å¤§ï¼Œæ£€ç´¢æ…¢
        self.long_term_memory = LongTermMemory(capacity=1000)
        
        # ç©ºé—´è®°å¿†ï¼šä½ç½®å’Œè·¯å¾„ä¿¡æ¯
        self.spatial_memory = SpatialMemory(world_size=config['world_size'])
    
    def store_experience(self, experience):
        """ç»éªŒå­˜å‚¨ç®—æ³•"""
        # 1. ç«‹å³å­˜å…¥å·¥ä½œè®°å¿†
        self.working_memory.add(experience)
        
        # 2. é‡è¦æ€§è¯„ä¼°
        importance = self._calculate_importance(experience)
        
        # 3. è¶…è¿‡é˜ˆå€¼åˆ™è½¬å…¥é•¿æœŸè®°å¿†
        if importance > 0.7:
            self.long_term_memory.consolidate(experience)
        
        # 4. ç©ºé—´ä¿¡æ¯å•ç‹¬å­˜å‚¨
        if 'location' in experience:
            self.spatial_memory.update_location_value(
                experience['location'], 
                experience['outcome']
            )
    
    def _calculate_importance(self, experience):
        """é‡è¦æ€§è®¡ç®—ç®—æ³•"""
        importance = 0.0
        
        # ç”Ÿå­˜ç›¸å…³ç»éªŒæ›´é‡è¦
        if experience['type'] == 'food_found':
            importance += 0.8
        elif experience['type'] == 'danger_avoided':
            importance += 0.9
        elif experience['type'] == 'reproduction_success':
            importance += 0.95
        elif experience['type'] == 'social_cooperation':
            importance += 0.6
        
        # æ–°é¢–æ€§åŠ åˆ†
        if not self._is_similar_experience_exists(experience):
            importance += 0.3
        
        # æƒ…æ„Ÿå¼ºåº¦åŠ åˆ†
        importance += abs(experience.get('emotional_value', 0)) * 0.2
        
        return min(importance, 1.0)
```

### 3. ğŸ­ è¡Œä¸ºç³»ç»Ÿæ¶æ„
```python
class BehaviorSystem:
    """åŠ¨æœºé©±åŠ¨çš„è¡Œä¸ºé€‰æ‹©ç³»ç»Ÿ"""
    
    def __init__(self, config):
        # å…­å¤§åŸºç¡€åŠ¨æœº
        self.motivations = {
            'survival': Motivation('survival', priority=1.0),      # ç”Ÿå­˜
            'energy': Motivation('energy', priority=0.9),         # èƒ½é‡è·å–
            'social': Motivation('social', priority=0.7),         # ç¤¾äº¤
            'exploration': Motivation('exploration', priority=0.6), # æ¢ç´¢
            'reproduction': Motivation('reproduction', priority=0.8), # ç¹æ®–
            'safety': Motivation('safety', priority=0.8)          # å®‰å…¨
        }
    
    def select_behavior(self, agent_state, world_state):
        """åŸºäºåŠ¨æœºå¼ºåº¦çš„è¡Œä¸ºé€‰æ‹©"""
        
        # 1. æ›´æ–°å„åŠ¨æœºå¼ºåº¦
        self._update_motivation_strengths(agent_state, world_state)
        
        # 2. è®¡ç®—å„è¡Œä¸ºçš„æ•ˆç”¨å€¼
        behavior_utilities = {}
        
        for behavior_name in self.available_behaviors:
            utility = self._calculate_behavior_utility(
                behavior_name, agent_state, world_state
            )
            behavior_utilities[behavior_name] = utility
        
        # 3. é€‰æ‹©æœ€é«˜æ•ˆç”¨çš„è¡Œä¸º
        best_behavior = max(behavior_utilities, key=behavior_utilities.get)
        
        # 4. æ·»åŠ éšæœºæ€§ï¼ˆæ¢ç´¢vsåˆ©ç”¨ï¼‰
        if random.random() < 0.1:  # 10%æ¢ç´¢æ¦‚ç‡
            best_behavior = random.choice(list(behavior_utilities.keys()))
        
        return best_behavior
    
    def _update_motivation_strengths(self, agent_state, world_state):
        """åŠ¨æœºå¼ºåº¦æ›´æ–°ç®—æ³•"""
        
        # ç”Ÿå­˜åŠ¨æœºï¼šèƒ½é‡è¶Šä½è¶Šå¼º
        energy_ratio = agent_state['energy'] / agent_state['max_energy']
        self.motivations['survival'].strength = 1.0 - energy_ratio
        
        # èƒ½é‡åŠ¨æœºï¼šä¸ç”Ÿå­˜åŠ¨æœºç›¸å…³ä½†æ›´æ—©æ¿€æ´»
        self.motivations['energy'].strength = max(0.2, 1.0 - energy_ratio * 1.2)
        
        # ç¤¾äº¤åŠ¨æœºï¼šåŸºäºå­¤ç‹¬æ„Ÿå’Œç¤¾äº¤å†å²
        loneliness = 1.0 - (agent_state['recent_interactions'] / 10.0)
        self.motivations['social'].strength = loneliness * 0.8
        
        # æ¢ç´¢åŠ¨æœºï¼šåŸºäºå¥½å¥‡å¿ƒå’Œç¯å¢ƒç†Ÿæ‚‰åº¦
        familiarity = agent_state['location_familiarity']
        self.motivations['exploration'].strength = 1.0 - familiarity
        
        # ç¹æ®–åŠ¨æœºï¼šå¹´é¾„ã€èƒ½é‡ã€å¥åº·çš„å‡½æ•°
        reproductive_readiness = (
            min(agent_state['age'] / 50.0, 1.0) *      # å¹´é¾„æˆç†Ÿåº¦
            (agent_state['energy'] / 100.0) *          # èƒ½é‡å……è¶³åº¦
            (agent_state['health'] / 100.0) *          # å¥åº·çŠ¶å†µ
            (1.0 - agent_state['offspring_count'] / 3.0) # ç¹æ®–é¥±å’Œåº¦
        )
        self.motivations['reproduction'].strength = reproductive_readiness
        
        # å®‰å…¨åŠ¨æœºï¼šåŸºäºå¨èƒæ„ŸçŸ¥
        threat_level = agent_state.get('perceived_threats', 0)
        self.motivations['safety'].strength = threat_level
```

---

## ğŸ”¬ å­¦ä¹ ä¸è¿›åŒ–æœºåˆ¶

### 1. ğŸ§¬ é—ä¼ ç®—æ³•
```python
class GeneticAlgorithm:
    """ç¥ç»ç½‘ç»œè¿›åŒ–ç®—æ³•"""
    
    def crossover(self, parent1_brain, parent2_brain):
        """äº¤å‰ç¹æ®–ç®—æ³•"""
        child_brain = NeuralBrain(parent1_brain.config)
        
        for layer_idx in range(len(parent1_brain.layers)):
            p1_weights = parent1_brain.layers[layer_idx].weights
            p2_weights = parent2_brain.layers[layer_idx].weights
            
            # å‡åŒ€äº¤å‰
            crossover_mask = np.random.random(p1_weights.shape) < 0.5
            child_weights = np.where(crossover_mask, p1_weights, p2_weights)
            
            # ç®—æœ¯äº¤å‰ï¼ˆæ··åˆï¼‰
            alpha = np.random.random()
            child_weights = alpha * p1_weights + (1 - alpha) * p2_weights
            
            child_brain.layers[layer_idx].weights = child_weights
        
        return child_brain
    
    def mutate(self, brain, mutation_rate=0.1):
        """çªå˜ç®—æ³•"""
        for layer in brain.layers:
            # æƒé‡çªå˜
            mutation_mask = np.random.random(layer.weights.shape) < mutation_rate
            mutation_values = np.random.normal(0, 0.1, layer.weights.shape)
            layer.weights += mutation_mask * mutation_values
            
            # ç»“æ„çªå˜ï¼ˆæ¦‚ç‡æä½ï¼‰
            if np.random.random() < 0.001:
                self._structural_mutation(layer)
    
    def _structural_mutation(self, layer):
        """ç»“æ„çªå˜ï¼šæ”¹å˜ç½‘ç»œæ‹“æ‰‘"""
        # æ·»åŠ /åˆ é™¤è¿æ¥
        if np.random.random() < 0.5:
            # æ·»åŠ è¿æ¥
            zero_weights = (layer.weights == 0)
            if np.any(zero_weights):
                i, j = np.where(zero_weights)
                idx = np.random.randint(len(i))
                layer.weights[i[idx], j[idx]] = np.random.normal(0, 0.1)
        else:
            # åˆ é™¤è¿æ¥
            nonzero_weights = (layer.weights != 0)
            if np.any(nonzero_weights):
                i, j = np.where(nonzero_weights)
                idx = np.random.randint(len(i))
                layer.weights[i[idx], j[idx]] = 0
```

### 2. ğŸ“ å¼ºåŒ–å­¦ä¹ 
```python
class ReinforcementLearning:
    """åŸºäºå¥–åŠ±çš„å­¦ä¹ ç³»ç»Ÿ"""
    
    def __init__(self):
        self.learning_rate = 0.01
        self.discount_factor = 0.95
        self.eligibility_traces = {}
    
    def update_policy(self, agent, action, reward, next_state):
        """æ”¿ç­–æ¢¯åº¦æ›´æ–°"""
        
        # è®¡ç®—æ—¶åºå·®åˆ†è¯¯å·®
        current_value = agent.brain.evaluate_state(agent.current_state)
        next_value = agent.brain.evaluate_state(next_state)
        
        td_error = reward + self.discount_factor * next_value - current_value
        
        # æ›´æ–°ç¥ç»ç½‘ç»œæƒé‡
        gradients = agent.brain.compute_policy_gradients(
            agent.current_state, action
        )
        
        for layer_idx, layer in enumerate(agent.brain.layers):
            layer.weights += (
                self.learning_rate * 
                td_error * 
                gradients[layer_idx]
            )
    
    def calculate_reward(self, agent, action, outcome):
        """å¥–åŠ±å‡½æ•°è®¾è®¡"""
        reward = 0.0
        
        # åŸºç¡€ç”Ÿå­˜å¥–åŠ±
        if agent.alive:
            reward += 0.1
        
        # èƒ½é‡å˜åŒ–å¥–åŠ±
        energy_change = outcome['energy_after'] - outcome['energy_before']
        reward += energy_change * 0.01
        
        # å¥åº·å˜åŒ–å¥–åŠ±  
        health_change = outcome['health_after'] - outcome['health_before']
        reward += health_change * 0.02
        
        # è¡Œä¸ºç‰¹å®šå¥–åŠ±
        if action['type'] == 'forage' and outcome['food_found']:
            reward += 1.0  # æˆåŠŸè§…é£Ÿå¤§å¥–åŠ±
        
        if action['type'] == 'social' and outcome['interaction_success']:
            reward += 0.5  # ç¤¾äº¤æˆåŠŸå¥–åŠ±
        
        if action['type'] == 'reproduction' and outcome['reproduction_success']:
            reward += 2.0  # ç¹æ®–æˆåŠŸå·¨å¤§å¥–åŠ±
        
        # æ¢ç´¢å¥–åŠ±
        if outcome['discovered_new_area']:
            reward += 0.3
        
        # ç”Ÿå­˜æ—¶é—´å¥–åŠ±
        reward += agent.age * 0.001
        
        return reward
```

---

## ğŸŒŸ æ¶Œç°è¡Œä¸ºè§‚å¯Ÿ

é€šè¿‡è¿™äº›å¤æ‚çš„ç®—æ³•äº¤äº’ï¼ŒCogvrsä¸­ä¼šè§‚å¯Ÿåˆ°ä»¥ä¸‹æ¶Œç°ç°è±¡ï¼š

### 1. ğŸ§  **ç¾¤ä½“æ™ºæ…§æ¶Œç°**
- **ä¿¡æ¯ä¼ æ’­**: æˆåŠŸè§…é£Ÿä½ç½®åœ¨ç¾¤ä½“ä¸­ä¼ æ’­
- **é›†ä½“å†³ç­–**: è¿ç§»æ–¹å‘ç”±å¤šä¸ªæ™ºèƒ½ä½“"æŠ•ç¥¨"å†³å®š
- **åˆ†å·¥åˆä½œ**: ä¸åŒæ™ºèƒ½ä½“ä¸“ç²¾ä¸åŒä»»åŠ¡

### 2. ğŸ˜ï¸ **ç¤¾ä¼šç»“æ„å½¢æˆ**
- **é¢†å¯¼è€…å‡ºç°**: é«˜ç¤¾äº¤èƒ½åŠ›æ™ºèƒ½ä½“æˆä¸ºä¸­å¿ƒèŠ‚ç‚¹
- **ç¾¤ä½“åˆ†åŒ–**: åŸºäºè¡Œä¸ºåå¥½çš„äºšç¾¤ä½“å½¢æˆ
- **æ–‡åŒ–ä¼ æ‰¿**: è¡Œä¸ºæ¨¡å¼åœ¨ä¸–ä»£é—´ä¼ é€’

### 3. ğŸ§¬ **è¿›åŒ–å‹åŠ›**
- **é€‚è€…ç”Ÿå­˜**: é«˜æ•ˆè§…é£Ÿç­–ç•¥è¢«ä¿ç•™
- **æ€§é€‰æ‹©**: ä¼˜ç§€åŸºå› é€šè¿‡ç¹æ®–ä¼˜åŠ¿ä¼ æ’­
- **ç¯å¢ƒé€‚åº”**: ç§ç¾¤ç‰¹å¾éšç¯å¢ƒå˜åŒ–è€Œæ¼”åŒ–

### 4. ğŸ­ **ä¸ªæ€§å‘å±•**
- **è¡Œä¸ºç‰¹åŒ–**: ä¸ªä½“å‘å±•ç‹¬ç‰¹çš„è¡Œä¸ºåå¥½
- **å­¦ä¹ èƒ½åŠ›åˆ†åŒ–**: ä¸åŒæ™ºèƒ½ä½“çš„å­¦ä¹ é€Ÿåº¦å·®å¼‚
- **é£é™©åå¥½**: ä¿å®ˆvså†’é™©çš„è¡Œä¸ºç­–ç•¥

è¿™ä¸ªç³»ç»Ÿå±•ç°äº†ä»ç®€å•è§„åˆ™åˆ°å¤æ‚è¡Œä¸ºçš„æ¶Œç°è¿‡ç¨‹ï¼Œæ˜¯ç ”ç©¶äººå·¥ç”Ÿå‘½å’Œæ„è¯†çš„ç†æƒ³å¹³å°ï¼ğŸš€